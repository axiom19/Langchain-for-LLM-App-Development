{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcjpOzeNjY5KHi8Nu+H9uI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axiom19/Langchain-for-LLM-App-Development/blob/main/Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain for LLM Application Development\n",
        "<hr>\n",
        "\n",
        "## Agents"
      ],
      "metadata": {
        "id": "8f6NcDyllKyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U langchain-community langchain langchain-openai langchain-experimental openai"
      ],
      "metadata": {
        "id": "D0do7WWKmbEv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.agents.agent_toolkits.python.base import create_python_agent\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import load_tools, initialize_agent\n",
        "from langchain.agents import AgentType"
      ],
      "metadata": {
        "id": "zmOlh0xamoyk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate the llm\n",
        "llm = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "gryHzCXgrG2_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install wikipedia"
      ],
      "metadata": {
        "id": "zAQvMiazsXjZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = load_tools([\"llm-math\", \"wikipedia\"], llm=llm)"
      ],
      "metadata": {
        "id": "XVAojC1qsOPB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize agent\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtNpt3k0sTrP",
        "outputId": "77bd5452-f19e-4f07-f949-061297f42bd8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-79a0fbde2bce>:3: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use :meth:`~Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.` instead.\n",
            "  agent = initialize_agent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent(\"What is the 25% of 300?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuJVDuJEs3sV",
        "outputId": "20250e68-98da-4822-e6bc-94ac39d8cb72"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-43d2a16e8e6d>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  agent(\"What is the 25% of 300?\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI should use the Calculator tool to calculate 25% of 300.\n",
            "Action: Calculator\n",
            "Action Input: 25% of 300\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 75.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 75.0\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the 25% of 300?', 'output': '75.0'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who is the Hari Singh Nalwa?\"\n",
        "agent(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDQyHViEs-XZ",
        "outputId": "e165cf8a-42c1-49ff-d94b-c348804dbf7b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI should use Wikipedia to find information about Hari Singh Nalwa.\n",
            "Action: wikipedia\n",
            "Action Input: Hari Singh Nalwa\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mPage: Hari Singh Nalwa\n",
            "Summary: Hari Singh Nalwa (29 April 1791 – 30 April 1837) was the commander-in-chief of the Sikh Khalsa Fauj, the army of the Sikh Empire. He is known for his role in the conquests of Kasur, Sialkot, Attock, Multan, Kashmir, Peshawar and Jamrud. Hari Singh Nalwa was responsible for expanding the frontier of Sikh Empire to beyond the Indus River right up to the mouth of the Khyber Pass. At the time of his death, Jamrud constituted the western boundary of the Empire.\n",
            "He served as governor of Kashmir, Peshawar and Hazara. He established a mint on behalf of the Sikh Empire to facilitate revenue collection in Kashmir and Peshawar.\n",
            "\n",
            "Page: Arjan Singh Nalwa\n",
            "Summary: Arjan Singh Nalwa (died 1848) was the youngest son of Sardar Hari Singh Nalwa and a minor jagirdar and rebel who refused to surrender to British rule in Punjab after the conclusion of the Second Anglo-Sikh War. He locked himself up in his estate in Gujranwala with 100 of his men and fought off a party sent by the Lahore Darbar to subdue him. Finally a company of Skinner's Horse was sent to bring him in, but he escaped his estate in time and died on the run a few years later. His property was confiscated by the British.\n",
            "\n",
            "Page: Akali Phula Singh\n",
            "Summary: Akali Phula Singh Nihang (born Phula Singh; 1 January 1761 – 14 March 1823) was an Akali Nihang Sikh leader. He was a saint soldier of the Khalsa Shaheedan Misl and head of the Budha Dal in the early 19th century. He was also a senior general in the Sikh Khalsa Army and commander of the irregular Nihang of the army. He played a role in uniting Sikh misls in Amritsar. He was not afraid  of the British who at many times ordered for his arrest but were not successful. During his later years he served for the Sikh Empire as a direct adviser to Maharaja Ranjit Singh. He remained an army general in many famous Sikh battles up until his martyrdom in the battle of Nowshera. He was admired by the local people and had a great influence over the land and his settlement was always open to help the poor and helpless. He was well known and was a humble unique leader and prestigious warrior with high character. He was also known for his effort to maintain the values of Gurmat and the Khalsa panth.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have gathered information about Hari Singh Nalwa from Wikipedia.\n",
            "Final Answer: Hari Singh Nalwa was the commander-in-chief of the Sikh Khalsa Fauj, known for his role in expanding the Sikh Empire's frontier and serving as governor of Kashmir, Peshawar, and Hazara.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Who is the Hari Singh Nalwa?',\n",
              " 'output': \"Hari Singh Nalwa was the commander-in-chief of the Sikh Khalsa Fauj, known for his role in expanding the Sikh Empire's frontier and serving as governor of Kashmir, Peshawar, and Hazara.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a python agent\n",
        "\n",
        "agent = create_python_agent(\n",
        "    llm=llm,\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "OlvA7YljuE3b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_list = [[\"Harrison\", \"Chase\"],\n",
        "                [\"Lang\", \"Chain\"],\n",
        "                [\"Dolly\", \"Too\"],\n",
        "                [\"Elle\", \"Elem\"],\n",
        "                [\"Geoff\",\"Fusion\"],\n",
        "                [\"Trance\",\"Former\"],\n",
        "                [\"Jen\",\"Ayai\"]\n",
        "               ]"
      ],
      "metadata": {
        "id": "TJrrVm5gurJ_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(f\"\"\"Sort these customers by \\\n",
        "last name and then first name \\\n",
        "and print the output: {customer_list}\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "8PK2x5Ksu5e2",
        "outputId": "ed067cdb-5e3f-4ef7-9788-f09bc9cf001f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-6f6c7c8c853d>:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  agent.run(f\"\"\"Sort these customers by \\\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mWe can use the `sorted()` function in Python to sort the list of customers based on last name first and then first name.\n",
            "Action: Python_REPL\n",
            "Action Input: customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\n",
            "sorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\n",
            "print(sorted_customers)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe customers are now sorted by last name and then first name.\n",
            "Final Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug = True\n",
        "agent.run(f\"\"\"Sort these customers by \\\n",
        "last name and then first name \\\n",
        "and print the output: {customer_list}\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3cRlTSr9u6xQ",
        "outputId": "bf00fda6-3e02-437f-9e2f-52a4b3d77397"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
            "  \"agent_scratchpad\": \"\",\n",
            "  \"stop\": [\n",
            "    \"\\nObservation:\",\n",
            "    \"\\n\\tObservation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [1.54s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"We can use the `sorted()` function in Python to sort the list of customers based on last name first and then first name.\\nAction: Python_REPL\\nAction Input: customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nprint(sorted_customers)\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"We can use the `sorted()` function in Python to sort the list of customers based on last name first and then first name.\\nAction: Python_REPL\\nAction Input: customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nprint(sorted_customers)\",\n",
            "            \"additional_kwargs\": {\n",
            "              \"refusal\": null\n",
            "            },\n",
            "            \"response_metadata\": {\n",
            "              \"token_usage\": {\n",
            "                \"completion_tokens\": 114,\n",
            "                \"prompt_tokens\": 328,\n",
            "                \"total_tokens\": 442,\n",
            "                \"completion_tokens_details\": {\n",
            "                  \"reasoning_tokens\": 0\n",
            "                }\n",
            "              },\n",
            "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
            "              \"system_fingerprint\": null,\n",
            "              \"finish_reason\": \"stop\",\n",
            "              \"logprobs\": null\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run-b2fa5662-885b-4173-a306-594722385e00-0\",\n",
            "            \"usage_metadata\": {\n",
            "              \"input_tokens\": 328,\n",
            "              \"output_tokens\": 114,\n",
            "              \"total_tokens\": 442\n",
            "            },\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 114,\n",
            "      \"prompt_tokens\": 328,\n",
            "      \"total_tokens\": 442,\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [1.56s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"We can use the `sorted()` function in Python to sort the list of customers based on last name first and then first name.\\nAction: Python_REPL\\nAction Input: customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nprint(sorted_customers)\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:Python_REPL] Entering Tool run with input:\n",
            "\u001b[0m\"customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\n",
            "sorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\n",
            "print(sorted_customers)\"\n",
            "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:Python_REPL] [4ms] Exiting Tool run with output:\n",
            "\u001b[0m\"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
            "  \"agent_scratchpad\": \"We can use the `sorted()` function in Python to sort the list of customers based on last name first and then first name.\\nAction: Python_REPL\\nAction Input: customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nprint(sorted_customers)\\nObservation: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\\n\\nThought:\",\n",
            "  \"stop\": [\n",
            "    \"\\nObservation:\",\n",
            "    \"\\n\\tObservation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL - A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:We can use the `sorted()` function in Python to sort the list of customers based on last name first and then first name.\\nAction: Python_REPL\\nAction Input: customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nprint(sorted_customers)\\nObservation: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\\n\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [891ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"I now know the final answer\\nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"I now know the final answer\\nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\",\n",
            "            \"additional_kwargs\": {\n",
            "              \"refusal\": null\n",
            "            },\n",
            "            \"response_metadata\": {\n",
            "              \"token_usage\": {\n",
            "                \"completion_tokens\": 61,\n",
            "                \"prompt_tokens\": 499,\n",
            "                \"total_tokens\": 560,\n",
            "                \"completion_tokens_details\": {\n",
            "                  \"reasoning_tokens\": 0\n",
            "                }\n",
            "              },\n",
            "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
            "              \"system_fingerprint\": null,\n",
            "              \"finish_reason\": \"stop\",\n",
            "              \"logprobs\": null\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run-ff69f5ae-0ac2-4705-b3da-fe535dcc8f22-0\",\n",
            "            \"usage_metadata\": {\n",
            "              \"input_tokens\": 499,\n",
            "              \"output_tokens\": 61,\n",
            "              \"total_tokens\": 560\n",
            "            },\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 61,\n",
            "      \"prompt_tokens\": 499,\n",
            "      \"total_tokens\": 560,\n",
            "      \"completion_tokens_details\": {\n",
            "        \"reasoning_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [892ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"I now know the final answer\\nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [2.46s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.debug = False"
      ],
      "metadata": {
        "id": "pmaFpajcvNP7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from langchain.agents import tool\n",
        " from datetime import date"
      ],
      "metadata": {
        "id": "ROJqXE1nvSr1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def time(text: str) -> str:\n",
        "    \"\"\"Returns todays date, use this for any \\\n",
        "    questions related to knowing todays date. \\\n",
        "    The input should always be an empty string, \\\n",
        "    and this function will always return todays \\\n",
        "    date - any date mathhemtical calculations \\\n",
        "    should occur outside this function.\"\"\"\n",
        "    return str(date.today())"
      ],
      "metadata": {
        "id": "U72T7sfUwnkU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(\n",
        "    tools + [time],\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        ")"
      ],
      "metadata": {
        "id": "eY3Qdiptww7o"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"whats the date today?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "OpMy5WKmw2ca",
        "outputId": "a7088060-2ed3-4faa-8b1f-e9313899360e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI should use the time function to get today's date.\n",
            "Action: time\n",
            "Action Input: \"\"\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3m2024-09-26\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have the current date.\n",
            "Final Answer: 2024-09-26\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2024-09-26'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YhpHGoCaw4S7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}