{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/mY82oOffwoSvMnavX725",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axiom19/Langchain-for-LLM-App-Development/blob/main/Chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain for LLM Application Development\n",
        "<hr>\n",
        "\n",
        "## Chains\n"
      ],
      "metadata": {
        "id": "Tgyjtjs08nYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install openai langchain\n",
        "# !pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "xbyq-LG89MTH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "6q0LB57B8ko8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the data from csv file\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"Data.csv\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "0Fa17WTF9huV",
        "outputId": "d5cd7311-53ae-476c-8f24-7cb7cad6eb11"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Product                                             Review\n",
              "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
              "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
              "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
              "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
              "4  Milk Frother Handheld\\n   I loved this product. But they only seem to l..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-241485ef-eae9-46fa-8263-656395e96879\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product</th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Queen Size Sheet Set</td>\n",
              "      <td>I ordered a king size set. My only criticism w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Waterproof Phone Pouch</td>\n",
              "      <td>I loved the waterproof sac, although the openi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Luxury Air Mattress</td>\n",
              "      <td>This mattress had a small hole in the top of i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pillows Insert</td>\n",
              "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Milk Frother Handheld\\n</td>\n",
              "      <td>I loved this product. But they only seem to l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-241485ef-eae9-46fa-8263-656395e96879')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-241485ef-eae9-46fa-8263-656395e96879 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-241485ef-eae9-46fa-8263-656395e96879');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-890d102d-4c22-4d78-8610-5590fd762fa6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-890d102d-4c22-4d78-8610-5590fd762fa6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-890d102d-4c22-4d78-8610-5590fd762fa6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"Product\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Queen Size Sheet Set\",\n          \"Waterproof Phone Pouch\",\n          \"L'Or Espresso Caf\\u00e9\\u00a0\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"I ordered a king size set. My only criticism would be that I wish seller would offer the king size set with 4 pillowcases. I separately ordered a two pack of pillowcases so I could have a total of four. When I saw the two packages, it looked like the color did not exactly match. Customer service was excellent about sending me two more pillowcases so I would have four that matched. Excellent! For the cost of these sheets, I am satisfied with the characteristics and coolness of the sheets.\",\n          \"I loved the waterproof sac, although the opening was made of a hard plastic. I don\\u2019t know if that would break easily. But I couldn\\u2019t turn my phone on, once it was in the pouch.\",\n          \"Je trouve le go\\u00fbt m\\u00e9diocre. La mousse ne tient pas, c'est bizarre. J'ach\\u00e8te les m\\u00eames dans le commerce et le go\\u00fbt est bien meilleur...\\nVieux lot ou contrefa\\u00e7on !?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import langchain\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "9XqG3ARq_Obx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the LLM\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "u5OT3q6X_v2n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the prompt template\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    template=\"What is the best name to describe a company that makes {product}?\"\n",
        ")"
      ],
      "metadata": {
        "id": "ur1tQlFn_OKu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLMChain\n",
        "<hr>\n",
        "LLMChain is the most basic type of chain"
      ],
      "metadata": {
        "id": "366_Wkk4Aj4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6F-V0rV_oN_",
        "outputId": "5e1436fb-4f2c-4c4b-e99c-7895bf12542a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-bddefa0a5dee>:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product = \"Queen Size Sheet Set\"\n",
        "chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nn9aan9M_5D-",
        "outputId": "b97ffad4-380e-4ad3-d271-781dee481621"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Royal Comfort Linens\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product2 = \"Natural Indigo Colored Fabric\"\n",
        "chain.run(product2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DPpdSlbw_-jf",
        "outputId": "a99d64f3-b49b-4e43-9d11-c3861323f3d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Indigo Naturals'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product3 = \"An AI software which makes a fashion marketing photoshoot with professional models and also creates small videos for the same\"\n",
        "chain.run(product3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yo0fBgBZAPX1",
        "outputId": "f1dc1003-1c05-40fe-f3f7-22ea9cd2f65a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"ModelAI Marketing Solutions\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential Chains\n",
        "<hr>\n",
        "Sequential chain is another type of chains. The idea is to combine multiple chains where the output of the one chain is the input of the next chain.\n",
        "\n",
        "There are two types of Sequential Chains:\n",
        "<li> SimpleSequentialChain : Single input/output\n",
        "<li> SequentialChain: multiple inputs/outputs"
      ],
      "metadata": {
        "id": "9aKhFWwoBQLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain"
      ],
      "metadata": {
        "id": "2HAoTRpLAeZc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 1\n",
        "first_prompt = prompt # using the previous prompt\n",
        "\n",
        "# chain 1\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
      ],
      "metadata": {
        "id": "-5ng19RIBxyq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 words description for the following company:{company_name}\"\n",
        ")\n",
        "\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ],
      "metadata": {
        "id": "8OkEZNFmCLUx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
        "                                                verbose=True)"
      ],
      "metadata": {
        "id": "OXV8GZUQCwdA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "XwDnMLBtC6Kc",
        "outputId": "d720ec34-fdaa-43d6-cd17-488d42a825f3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36;1m\u001b[1;3mRoyaltyLinens\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mLuxury linens fit for royalty, offering exquisite bedding, towels, and table linens for a regal touch to any home decor.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Luxury linens fit for royalty, offering exquisite bedding, towels, and table linens for a regal touch to any home decor.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain(product2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWw8yrPWDB1K",
        "outputId": "12278291-7cf9-4c9e-b46d-5853893e7900"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-c490c297bdb3>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "  overall_simple_chain(product2)\n",
            "WARNING:langchain_core.callbacks.manager:Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36;1m\u001b[1;3m\"Indigo Nature\"\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mIndigo Nature is a sustainable beauty company offering organic, eco-friendly skincare products that harness the power of natural ingredients.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Natural Indigo Colored Fabric',\n",
              " 'output': 'Indigo Nature is a sustainable beauty company offering organic, eco-friendly skincare products that harness the power of natural ingredients.'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SimpleSequentialChain works well when we have a single input and a single output. Otherwise we can use just **SequentialChain**"
      ],
      "metadata": {
        "id": "FOpro6J4Dayg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain"
      ],
      "metadata": {
        "id": "RSiYLoQbDJxh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.9)\n",
        "\n",
        "# prompt template 1: translate to english\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to english:{Review}\"\n",
        ")\n",
        "\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
        "                     output_key=\"English_Review\"\n",
        "                     )"
      ],
      "metadata": {
        "id": "wnsaSZISDqou"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence:{English_Review}\"\n",
        ")\n",
        "\n",
        "# chain 2: input= English_Review and output= summary\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
        "                     output_key=\"summary\"\n",
        "                     )"
      ],
      "metadata": {
        "id": "NL9auzDBECdK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 3: translate to english\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:\\n\\n{Review}\"\n",
        ")\n",
        "\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"language\"\n",
        "                       )"
      ],
      "metadata": {
        "id": "XvRLaat1EIkw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 4: follow up message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "\n",
        "# chain 4: input= summary, language and output= followup_message\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
        "                    output_key=\"followup_message\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "-Jj7Y0jMESPX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "o0mhQdhfEmkc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = df.Review[5]\n",
        "review"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "YNrYr8EoFMqI",
        "outputId": "fde3509c-81d8-4696-8c3b-cbc2a9f57ea8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsvvfhs0FO84",
        "outputId": "2c91b688-b1f6-409e-ff5f-16edc3aad484"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
              " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\",\n",
              " 'summary': 'The reviewer found the taste of the product to be underwhelming and questioned whether it was an old batch or counterfeit.',\n",
              " 'followup_message': \"Merci pour votre commentaire honnête. Nous sommes désolés d'apprendre que vous avez trouvé le goût du produit décevant. Nous pouvons vous assurer que nos produits sont frais et authentiques. Cependant, nous apprécions vos préoccupations et nous prendrons des mesures pour nous assurer que nos produits répondent toujours à vos attentes élevées. Merci de nous avoir informés de votre expérience.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Router Chain\n",
        "<hr>\n",
        "A router chain in LangChain dynamically routes user inputs to the most appropriate sub-chain or model based on the content of the input."
      ],
      "metadata": {
        "id": "i9Tt3xRUFuGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "u2qLkao7FWGT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "Wnbc8wqFGLz7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing other chains that we need\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "ncjvfxVrGa-O"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "P-UtQ_OEGtZM"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we create destination chain where each chain is a LLMChain\n",
        "\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ],
      "metadata": {
        "id": "KF2oSGOaG65h"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "metadata": {
        "id": "3OrH_IQlHgLU"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ],
      "metadata": {
        "id": "lV3oEF4GILPJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "metadata": {
        "id": "PzbbHGFzIRPY"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "metadata": {
        "id": "Y6nI_rArIhHH"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnVwwBMHIZ8n",
        "outputId": "cec7db37-91d9-4b9e-865b-4d239eb2a12f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-ae9cc652db54>:1: LangChainDeprecationWarning: Use RunnableLambda to select from multiple prompt templates. See example in API reference: https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html\n",
            "  chain = MultiPromptChain(router_chain=router_chain,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"What is black body radiation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "lpz6m1R1Ih6S",
        "outputId": "b64f4204-cebb-4747-b9b6-6709d09b24a2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Black body radiation is the electromagnetic radiation emitted by a perfect absorber and emitter of radiation, known as a black body. A black body absorbs all radiation that falls on it and emits radiation across the entire electromagnetic spectrum. The spectrum of black body radiation is continuous and depends only on the temperature of the black body. This phenomenon is described by Planck's law, which states that the intensity of radiation emitted by a black body at a given wavelength is proportional to the temperature of the body and the wavelength raised to the fifth power.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"How did the Sikh Empire fall?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "y6rpMUG_Ilkb",
        "outputId": "81ed70f0-9e87-4a80-b367-aa174fc78373"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "History: {'input': 'How did the Sikh Empire fall?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Sikh Empire, also known as the Sikh Kingdom of the Punjab, fell due to a combination of internal conflicts, external invasions, and the changing political landscape of the region.\\n\\nOne of the main factors that led to the downfall of the Sikh Empire was internal strife and power struggles among the Sikh leaders after the death of Maharaja Ranjit Singh in 1839. The lack of a strong and capable leader to succeed him resulted in a fragmented and weakened empire.\\n\\nAdditionally, the British East India Company, seeking to expand its influence in the region, took advantage of the internal divisions within the Sikh Empire and launched the First Anglo-Sikh War in 1845. Despite initial Sikh victories, the British ultimately emerged victorious and annexed significant territories of the Sikh Empire.\\n\\nThe Second Anglo-Sikh War in 1848 further weakened the Sikh Empire, leading to the annexation of the Punjab by the British in 1849. The British established direct control over the region, effectively bringing an end to the Sikh Empire.\\n\\nOverall, the fall of the Sikh Empire can be attributed to a combination of internal conflicts, external invasions, and the superior military and political power of the British East India Company.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ]
}